{
  "description": "ViT encoder with patch embed + 1 Transformer layer (B=1, 512x512, patch=16, D=2560) using OPTIMIZED attention algorithm",
  "tensors": [
    {"name": "img", "shape": [3, 512, 512], "bits": 16, "device": "dram"},
    {"name": "W_embed", "shape": [2560, 2560], "bits": 4, "device": "rram"},
    {"name": "tokens", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "W_q_0", "shape": [2560, 2560], "bits": 4, "device": "rram"},
    {"name": "W_k_0", "shape": [2560, 2560], "bits": 4, "device": "rram"},
    {"name": "W_v_0", "shape": [2560, 2560], "bits": 4, "device": "rram"},
    {"name": "Q_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "K_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "V_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "attn_scores_0", "shape": [1024, 1024], "bits": 16, "device": "dram"},
    {"name": "attn_probs_0", "shape": [1024, 1024], "bits": 16, "device": "dram"},
    {"name": "attn_out_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "norm1_out_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "resid1_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "norm2_out_0", "shape": [1, 1024, 2560], "bits": 16, "device": "dram"},
    {"name": "W1_0", "shape": [2560, 10240], "bits": 4, "device": "rram"},
    {"name": "W2_0", "shape": [10240, 2560], "bits": 4, "device": "rram"},
    {"name": "ff1_0", "shape": [1024, 10240], "bits": 16, "device": "dram"},
    {"name": "ff1_act_0", "shape": [1024, 10240], "bits": 16, "device": "dram"},
    {"name": "ff2_0", "shape": [1024, 2560], "bits": 16, "device": "dram"},
    {"name": "resid2_0", "shape": [1, 1024, 2560], "bits": 16, "device": "dram"},
    {"name": "Q_remote", "shape": [1024, 2560], "bits": 16, "device": "dram"}
  ],
  "ops": [
    {"type": "PatchEmbed", "input_img": "img", "patch_w": 16, "patch_h": 16, "out": "tokens", "weight_name": "W_embed"},
    {"type": "LayerNorm", "A": "tokens", "C": "norm1_out_0"},

    {"type": "UCIeOp", "size_bits": 41943040},

    {
      "type": "ParallelOps",
      "branches": [
        {"type": "MatMul", "A": "norm1_out_0", "B": "W_q_0", "C": "Q_0"},
        {"type": "MatMul", "A": "norm1_out_0", "B": "W_k_0", "C": "K_0"},
        {"type": "MatMul", "A": "norm1_out_0", "B": "W_v_0", "C": "V_0"}
      ]
    },

    {"type": "UCIeOp", "size_bits": 41943040},

    {"type": "MatMul", "A": "Q_0", "B": "K_0", "C": "attn_scores_0", "transpose_B": true},
    {"type": "SoftmaxOp", "input": "attn_scores_0", "axis": -1, "output": "attn_probs_0"},
    {"type": "MatMul", "A": "attn_probs_0", "B": "V_0", "C": "attn_out_0"},
    {"type": "AddOp", "A": "attn_out_0", "B": "norm1_out_0", "C": "resid1_0"},
    {"type": "LayerNorm", "A": "resid1_0", "C": "norm2_out_0"},
    {"type": "MatMul", "A": "resid1_0", "B": "W1_0", "C": "ff1_0"},
    {"type": "GeluOp", "A": "ff1_0", "C": "ff1_act_0"},
    {"type": "MatMul", "A": "ff1_act_0", "B": "W2_0", "C": "ff2_0"},
    {"type": "AddOp", "A": "ff2_0", "B": "resid1_0", "C": "resid2_0"},
    {"type": "UCIeOp", "size_bits": 41943040}
  ]
}